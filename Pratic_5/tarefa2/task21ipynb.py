# -*- coding: utf-8 -*-
"""task21ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13F3IwtFcaUxUMnuoMv6bXPB4lQLu303j
"""



#!apt-get update
#!apt-get install openjdk-8-jdk-headless
#!wget -c https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
#!tar -xzf spark-3.3.1-bin-hadoop3.tgz
#!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import SparkSession
sc = SparkSession.builder.master("local[1]") \
.appName('SimpleApp') \
.getOrCreate() \
#.sparkContext


df1 = sc.read.json("2018_04_21_linhas.json")
#df1.show()
rdd_linha = df1.rdd.map(lambda x: (x["COD"], x["NOME"]))
#for num_linha in rdd_linhas.collect():
#    print(num_linha)

df2 = sc.read.json("2018_04_21_tabelaLinha.json")
#df2.show()
rdd_tabela = df2.rdd.map(lambda x: (x["COD"], x["HORA"], x["NUM"], x["PONTO"]))
#for num_linha in rdd_tabela.collect():
#    print(num_linha)

n_linha_onibus = input("Digite o número da linha de ônibus desejada: ")

filter_linha = rdd_linha.filter(lambda x: n_linha_onibus in x).collect()
print(filter_linha)

filter_tabela = rdd_tabela.filter(lambda x: n_linha_onibus in x)

for num_linha in filter_tabela.collect():
    print(num_linha)

#!apt-get update
#!apt-get install openjdk-8-jdk-headless
#!wget -c https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
#!tar -xzf spark-3.3.1-bin-hadoop3.tgz
#!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import SparkSession
sc = SparkSession.builder.master("local[1]") \
.appName('SimpleApp') \
.getOrCreate() \
#.sparkContext


df1 = sc.read.json("2018_04_21_linhas.json")
#df1.show()
rdd_linha = df1.rdd.map(lambda x: (x["COD"], x["NOME"]))
#for num_linha in rdd_linhas.collect():
#    print(num_linha)

df2 = sc.read.json("2018_04_21_tabelaLinha.json")
#df2.show()
rdd_tabela = df2.rdd.map(lambda x: (x["COD"], x["HORA"], x["NUM"], x["PONTO"]))
#for num_linha in rdd_tabela.collect():
#    print(num_linha)

n_linha_onibus = input("Digite o número da linha de ônibus desejada: ")

filter_linha = rdd_linha.filter(lambda x: n_linha_onibus in x).collect()
print(filter_linha)

filter_tabela = rdd_tabela.filter(lambda x: n_linha_onibus in x)

for num_linha in filter_tabela.collect():
    print(num_linha)

#!apt-get update
#!apt-get install openjdk-8-jdk-headless
#!wget -c https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
#!tar -xzf spark-3.3.1-bin-hadoop3.tgz
#!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import SparkSession
sc = SparkSession.builder.master("local[1]") \
.appName('SimpleApp') \
.getOrCreate() \
#.sparkContext


df1 = sc.read.json("2018_04_21_linhas.json")
#df1.show()
rdd_linha = df1.rdd.map(lambda x: (x["COD"], x["NOME"]))
#for num_linha in rdd_linhas.collect():
#    print(num_linha)

df2 = sc.read.json("2018_04_21_tabelaLinha.json")
#df2.show()
rdd_tabela = df2.rdd.map(lambda x: (x["COD"], x["HORA"], x["NUM"], x["PONTO"]))
#for num_linha in rdd_tabela.collect():
#    print(num_linha)

n_linha_onibus = input("Digite o número da linha de ônibus desejada: ")

filter_linha = rdd_linha.filter(lambda x: n_linha_onibus in x).collect()


filter_tabela = rdd_tabela.filter(lambda x: n_linha_onibus in x)

for num_linha in filter_tabela.collect():
    print(num_linha)

#!apt-get update
#!apt-get install openjdk-8-jdk-headless
#!wget -c https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
#!tar -xzf spark-3.3.1-bin-hadoop3.tgz
#!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"

import findspark
findspark.init()

import pandas as pd

from pyspark.sql import SparkSession
sc = SparkSession.builder.master("local[1]") \
.appName('SimpleApp') \
.getOrCreate() \
#.sparkContext

#Ler o arquivo json das linhas
df1 = sc.read.json("2018_04_21_linhas.json")
#df1.show()

#Transformar em RDD e mapear o nome da linha
rdd_linha = df1.rdd.map(lambda x: (x["NOME"]))
#for num_linha in rdd_linhas.collect():
#    print(num_linha)

#Ler o arquivo json com a tabela de pontos e horários
df2 = sc.read.json("2018_04_21_tabelaLinha.json")
#df2.show()


rdd_tabela = df2.rdd.map(lambda x: (x["HORA"], x["NUM"], x["PONTO"]))
#for num_linha in rdd_tabela.collect():
#    print(num_linha)

rdd = rdd_linha.cartesian(rdd_tabela)

n_linha_onibus = input("Digite o número da linha de ônibus desejada: ")

linhas = rdd.filter(lambda x: n_linha_onibus in x[1])

print("(NOME DA LINHA, HORÁRIO, NÚMERO, PONTO(PARADA))")
for linha in linhas.collect():
    print(linha)

#!apt-get update
#!apt-get install openjdk-8-jdk-headless
#!wget -c https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
#!tar -xzf spark-3.3.1-bin-hadoop3.tgz
#!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import SparkSession
sc = SparkSession.builder.master("local[1]") \
.appName('SimpleApp') \
.getOrCreate() \
#.sparkContext

#Ler o arquivo json das linhas
df1 = sc.read.json("2018_04_21_linhas.json")
#df1.show()

#Transformar em RDD e mapear o nome da linha
rdd_linha = df1.rdd.map(lambda x: (x["NOME"]))
#for num_linha in rdd_linhas.collect():
#    print(num_linha)

#Ler o arquivo json com a tabela de pontos e horários
df2 = sc.read.json("2018_04_21_tabelaLinha.json")
#df2.show()

#Transformar em RDD e mapear o horário, número e ponto de parada
rdd_tabela = df2.rdd.map(lambda x: (x["HORA"], x["NUM"], x["PONTO"]))
#for num_linha in rdd_tabela.collect():
#    print(num_linha)

#Usar a função cartesian() para unir os dois RDD's com os mapeamentos dos campos
rdd = rdd_linha.cartesian(rdd_tabela)

#Pegar o número da linha do ônibus que o usuário deseja
n_linha_onibus = input("Digite o número da linha de ônibus desejada: ")

#filtar no segundo RDD o número da linha do ônibus
linhas = rdd.filter(lambda x: n_linha_onibus in x[1])

#impressão solicitada pelo usuário a partir do número da linha de ônibus
print("(NOME DA LINHA, HORÁRIO, NÚMERO, PONTO(PARADA))")
for linha in linhas.collect():
    print(linha)